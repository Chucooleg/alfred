{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['ALFRED_ROOT'] = '/root/data/home/hoyeung/alfred/'\n",
    "os.environ['ALFRED_ROOT'] = '/root/home/legg/alfred/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['ALFRED_ROOT']))\n",
    "sys.path.append(os.path.join(os.environ['ALFRED_ROOT'], 'models'))\n",
    "\n",
    "# from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "import torch\n",
    "import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "from data.preprocess import Dataset\n",
    "from importlib import import_module, reload\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "from collections import Counter\n",
    "# from models.utils.helper_utils import optimizer_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# settings\n",
    "# args.data = '/root/data_alfred/json_data_augmentation_20200820'\n",
    "# args.data = '/media/legg/data/alfred_data/json_data_augmentation_20200820'\n",
    "args.pp_folder = 'pp'\n",
    "args.pframe = 300\n",
    "args.fast_epoch = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess original train splits oct21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP preprocessing. Already preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess augmented splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(os.path.join('/media/legg/data/alfred_data/json_feat_2.1.0_backup_20200826_agent_training', \"%s.vocab\" % args.pp_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': Vocab(2360), 'action_low': Vocab(15), 'action_high': Vocab(93)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('augmentation', 7570)]\n"
     ]
    }
   ],
   "source": [
    "args.splits = '/root/data_alfred/splits/sample_failed_20200820_filtered.json'\n",
    "# args.splits = '/root/data_alfred/splits/sample_failed_toy.json'\n",
    "\n",
    "with open(args.splits, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "print([(k,len(splits[k])) for k in splits.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'look_at_obj_in_light-BaseballBat-None-DeskLamp-301/trial_T20200814_164125_595727',\n",
       " 'repeat_idx': 0,\n",
       " 'full_traj_success': False,\n",
       " 'collected_subgoals': 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits['augmentation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (7570 of 7570) |####################| Elapsed Time: 0:01:38 Time:  0:01:38\n"
     ]
    }
   ],
   "source": [
    "args.fast_epoch = False\n",
    "dataset = Dataset(args, vocab)\n",
    "dataset.preprocess_splits_augmentation(splits, baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (7570 of 7570) |####################| Elapsed Time: 0:01:29 Time:  0:01:29\n"
     ]
    }
   ],
   "source": [
    "args.fast_epoch = False\n",
    "dataset = Dataset(args, vocab)\n",
    "dataset.preprocess_splits_augmentation(splits, baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in existing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('train', 21023), ('valid_seen', 820), ('valid_unseen', 821)]\n"
     ]
    }
   ],
   "source": [
    "original_alfred_splits_p = '/root/home/legg/data_alfred/splits/oct21.json'\n",
    "\n",
    "with open(original_alfred_splits_p, 'r') as f:\n",
    "    original_alfred_splits = json.load(f)\n",
    "    \n",
    "print([(k,len(original_alfred_splits[k])) for k in original_alfred_splits.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('augmentation', 7570)]\n"
     ]
    }
   ],
   "source": [
    "augmented_splits_p = '/root/home/legg/data_alfred/splits/sample_failed_20200820_filtered.json'\n",
    "\n",
    "with open(augmented_splits_p, 'r') as f:\n",
    "    augmented_splits = json.load(f)\n",
    "\n",
    "print([(k,len(augmented_splits[k])) for k in augmented_splits.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Unseen envs to be filtered out from failed trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided by Mohit\n",
    "envs_train = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 426, 427, 428, 429, 430]\n",
    "envs_valid_seen = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 201, 202, 203, 204, 205, 206, 207, 212, 213, 214, 216, 218, 222, 223, 224, 225, 227, 229, 230, 301, 302, 303, 304, 305, 309, 310, 311, 313, 314, 316, 318, 320, 323, 324, 326, 327, 328, 329, 330, 401, 402, 403, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 417, 418, 419, 422, 423, 426, 427, 428, 429]\n",
    "envs_valid_unseen = [10, 219, 308, 424]\n",
    "envs_test_seen = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 220, 221, 222, 223, 224, 225, 227, 228, 230, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 426, 427, 428, 429, 430]\n",
    "envs_test_unseen = [215, 226, 29, 315, 325, 404, 425, 9]\n",
    "\n",
    "envs_train = set(envs_train)\n",
    "envs_valid_seen = set(envs_valid_seen)\n",
    "envs_valid_unseen = set(envs_valid_unseen)\n",
    "envs_test_seen = set(envs_test_seen)\n",
    "envs_test_unseen = set(envs_test_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs_augmentation = []\n",
    "\n",
    "for task in augmented_splits['augmentation']:\n",
    "    env_id = task['task'].split('/')[-2].split('-')[-1]\n",
    "    envs_augmentation.append(int(env_id))\n",
    "\n",
    "envs_augmentation = set(envs_augmentation)\n",
    "len(envs_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10, 219, 308, 424}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs_augmentation & envs_valid_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9, 29, 215, 226, 315, 325, 404, 425}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs_augmentation & envs_test_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(envs_valid_unseen|envs_test_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Annotate all failures with Explainer and with Baseline.  Retrain agent and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unseen(split, unseen_env_ids):\n",
    "    out = []\n",
    "    for task in split:\n",
    "        env_id = int(task['task'].split('/')[-2].split('-')[-1])\n",
    "        if not env_id in unseen_env_ids:\n",
    "            out.append(task)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('train', 21023), ('valid_seen', 820), ('valid_unseen', 821), ('augmentation', 6949)]\n"
     ]
    }
   ],
   "source": [
    "experiment_1_splits = {}\n",
    "\n",
    "for k in original_alfred_splits.keys():\n",
    "    experiment_1_splits[k] = original_alfred_splits[k] \n",
    "\n",
    "experiment_1_splits['augmentation'] = filter_unseen(\n",
    "    split=augmented_splits['augmentation'], unseen_env_ids=envs_valid_unseen|envs_test_unseen)\n",
    "\n",
    "print([(k,len(experiment_1_splits[k])) for k in experiment_1_splits.keys()])\n",
    "\n",
    "# before correction:\n",
    "# [('tests_seen', 1533), ('tests_unseen', 1529), ('train', 21023), ('valid_seen', 820), ('valid_unseen', 821), \n",
    "# ('augmentation', 7570)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('train', 21023), ('valid_seen', 820), ('valid_unseen', 821), ('augmentation', 6949)]\n"
     ]
    }
   ],
   "source": [
    "experiment_1_split_path = '/root/home/legg/data_alfred/splits/data_augmentation_experiment1_20200826_redo20210608.json'\n",
    "with open(experiment_1_split_path, 'w') as f:\n",
    "    json.dump(experiment_1_splits, f)\n",
    "    \n",
    "with open(experiment_1_split_path, 'r') as f:\n",
    "    dum = json.load(f)\n",
    "\n",
    "print([(k,len(dum[k])) for k in dum.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrain agent on only half the original training data and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 5990, 6: 282, 5: 156, 4: 145, 1: 1})\n",
      "21023\n",
      "6574\n"
     ]
    }
   ],
   "source": [
    "# distribution -- number of annotations per task\n",
    "counter = dict()\n",
    "\n",
    "for task in original_alfred_splits['train']:\n",
    "    if task['task'] not in counter:\n",
    "        counter[task['task']] = 0\n",
    "    counter[task['task']] += 1\n",
    "    \n",
    "print(Counter(counter.values()))\n",
    "print(sum([k*v for k,v in Counter(counter.values()).items()]))\n",
    "print(len(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pick_and_place_simple-WineBottle-None-Shelf-7/trial_T20190907_200104_945677'\n",
      " 'pick_cool_then_place_in_recep-Bowl-None-Cabinet-24/trial_T20190909_112909_923737'\n",
      " 'pick_heat_then_place_in_recep-AppleSliced-None-DiningTable-17/trial_T20190908_095621_618627'\n",
      " 'pick_and_place_simple-TennisRacket-None-SideTable-310/trial_T20190907_040920_796378'\n",
      " 'pick_two_obj_and_place-CreditCard-None-Desk-313/trial_T20190909_104526_715846'\n",
      " 'pick_and_place_simple-Candle-None-Cart-430/trial_T20190909_055319_119903'\n",
      " 'pick_heat_then_place_in_recep-PotatoSliced-None-Fridge-22/trial_T20190908_185723_946017'\n",
      " 'pick_and_place_with_movable_recep-RemoteControl-Box-Sofa-220/trial_T20190907_023003_992808'\n",
      " 'pick_heat_then_place_in_recep-Egg-None-SinkBasin-25/trial_T20190907_024723_698757'\n",
      " 'pick_cool_then_place_in_recep-Plate-None-Cabinet-25/trial_T20190908_102731_668233']\n",
      "3287\n"
     ]
    }
   ],
   "source": [
    "# randomly select half of the tasks\n",
    "np.random.seed(42)\n",
    "keep_tasks = np.random.choice(list(counter.keys()), size=len(counter)//2, replace=False)\n",
    "print(keep_tasks[:10])\n",
    "keep_tasks = set(keep_tasks)\n",
    "print(len(keep_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10524\n"
     ]
    }
   ],
   "source": [
    "# create new train split\n",
    "half_train_split = []\n",
    "ct = 0\n",
    "for task in original_alfred_splits['train']:\n",
    "    if task['task'] in keep_tasks:\n",
    "        ct += 1\n",
    "        half_train_split.append(task)\n",
    "        \n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('valid_seen', 820), ('valid_unseen', 821), ('train', 10524)]\n"
     ]
    }
   ],
   "source": [
    "# assemble splits\n",
    "experiment_2_splits = {}\n",
    "\n",
    "for k in original_alfred_splits.keys():\n",
    "    if k != 'train':\n",
    "        experiment_2_splits[k] = original_alfred_splits[k] \n",
    "\n",
    "experiment_2_splits['train'] = half_train_split\n",
    "\n",
    "print([(k,len(experiment_2_splits[k])) for k in experiment_2_splits.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('valid_seen', 820), ('valid_unseen', 821), ('train', 10524)]\n"
     ]
    }
   ],
   "source": [
    "# save out\n",
    "experiment_2_split_path = '/media/legg/data/alfred_data/splits/data_augmentation_experiment2_20200831.json'\n",
    "\n",
    "with open(experiment_2_split_path, 'w') as f:\n",
    "    json.dump(experiment_2_splits, f)\n",
    "    \n",
    "with open(experiment_2_split_path, 'r') as f:\n",
    "    dum = json.load(f)\n",
    "\n",
    "print([(k,len(dum[k])) for k in dum.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label the other half of the training data with the Explainer, retrain agent, eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get list of tasks that were labeled by the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = '/media/legg/data/alfred_data/exp_all/model_seq2seq_per_subgoal,name_v2_epoch_40_obj_instance_enc_max_pool_dec_aux_loss_weighted_bce_1to2_sample_sentences/'\n",
    "\n",
    "split = 'train'\n",
    "temperature = '0.75'\n",
    "pred_f = os.path.join(dout, '{}_sampled.temperature_{}.preds.json'.format(split, temperature))\n",
    "with open(pred_f, 'r') as f:\n",
    "    explainer_pred = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_available_list_exp = []\n",
    "\n",
    "for k in explainer_pred.keys():\n",
    "    tk = '/'.join(explainer_pred[k]['root'].split('/')[-2:])\n",
    "    pred_available_list_exp.append(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = '/media/legg/data/alfred_data/exp_all/model_seq2seq_per_subgoal,name_v2_epoch_35_baseline_sample_sentences/'\n",
    "\n",
    "split = 'train'\n",
    "temperature = '0.75'\n",
    "pred_f = os.path.join(dout, '{}_sampled.temperature_{}.preds.json'.format(split, temperature))\n",
    "with open(pred_f, 'r') as f:\n",
    "    baseline_pred = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_available_list_base = []\n",
    "\n",
    "for k in baseline_pred.keys():\n",
    "    tk = '/'.join(baseline_pred[k]['root'].split('/')[-2:])\n",
    "    pred_available_list_base.append(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6505"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(pred_available_list_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6505"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(pred_available_list_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter out the half used in experiment 2; But labeled by Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10394\n"
     ]
    }
   ],
   "source": [
    "# create new train split\n",
    "other_half_train_split = []\n",
    "ct = 0\n",
    "for task in original_alfred_splits['train']:\n",
    "    if (task['task'] not in keep_tasks) and (task['task'] in set(pred_available_list_exp)):\n",
    "        ct += 1\n",
    "        task['use_autogeneration'] = True\n",
    "        other_half_train_split.append(task)\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Merge the half used in experiment 2 and the other half we tagged as autogenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10524\n",
      "10394\n"
     ]
    }
   ],
   "source": [
    "print(len(half_train_split))\n",
    "print(len(other_half_train_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert no overlap\n",
    "assert not set([t['task'] for t in half_train_split]) & set([t['task'] for t in other_half_train_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('valid_seen', 820), ('valid_unseen', 821), ('train', 20918)]\n"
     ]
    }
   ],
   "source": [
    "# assemble splits\n",
    "experiment_3_splits = {}\n",
    "\n",
    "for k in original_alfred_splits.keys():\n",
    "    if k != 'train':\n",
    "        experiment_3_splits[k] = original_alfred_splits[k] \n",
    "\n",
    "experiment_3_splits['train'] = half_train_split + other_half_train_split\n",
    "\n",
    "print([(k,len(experiment_3_splits[k])) for k in experiment_3_splits.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests_seen', 1533), ('tests_unseen', 1529), ('valid_seen', 820), ('valid_unseen', 821), ('train', 20918)]\n"
     ]
    }
   ],
   "source": [
    "# save out\n",
    "experiment_3_split_path = '/media/legg/data/alfred_data/splits/data_augmentation_experiment3_20200903.json'\n",
    "\n",
    "# original_alfred_splits has been modified inplace\n",
    "with open(experiment_3_split_path, 'w') as f:\n",
    "    json.dump(experiment_3_splits, f)\n",
    "    \n",
    "with open(experiment_3_split_path, 'r') as f:\n",
    "    dum = json.load(f)\n",
    "\n",
    "print([(k,len(dum[k])) for k in dum.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/media/legg/data/alfred_data/json_feat_2.1.0_backup_20200826_agent_training'\n",
    "# data_p = os.path.join(p, 'pick_cool_then_place_in_recep-LettuceSliced-None-DiningTable-17/trial_T20190909_070538_437648/pp/', 'aug_explainer_0_T20200810_1829.json')\n",
    "data_p = os.path.join(p, 'pick_cool_then_place_in_recep-LettuceSliced-None-DiningTable-17/trial_T20190909_070538_437648/pp/', 'ann_0.json')\n",
    "\n",
    "\n",
    "with open(data_p, 'r') as f:\n",
    "    data1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/media/legg/data/alfred_data/json_feat_2.1.0_backup_20200826_agent_training'\n",
    "data_p = os.path.join(p, 'pick_cool_then_place_in_recep-LettuceSliced-None-DiningTable-17/trial_T20190909_070538_437648/pp/', 'aug_explainer_0_T20200810_1829.json')\n",
    "# data_p = os.path.join(p, 'pick_cool_then_place_in_recep-LettuceSliced-None-DiningTable-17/trial_T20190909_070538_437648/pp/', 'ann_0.json')\n",
    "\n",
    "\n",
    "with open(data_p, 'r') as f:\n",
    "    data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
