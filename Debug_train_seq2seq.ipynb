{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training cycle debugging\n",
    "\n",
    "Basically run code copied from train_seq2seq.py in this notebook to catch bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ALFRED_ROOT'] = '/root/data/home/hoyeung/alfred/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['ALFRED_ROOT']))\n",
    "sys.path.append(os.path.join(os.environ['ALFRED_ROOT'], 'models'))\n",
    "\n",
    "# from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "import torch\n",
    "import pprint\n",
    "import json\n",
    "from data.preprocess import Dataset\n",
    "from importlib import import_module, reload\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "from models.utils.helper_utils import optimizer_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the default flags present in train_seq2seq.py\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# settings\n",
    "args.seed = 123\n",
    "args.data = '/root/data_alfred/json_feat_2.1.0'\n",
    "args.splits = '/root/data_alfred/splits/may17.json'\n",
    "args.object_vocab = 'objects_20200522'\n",
    "args.preprocess = False #!\n",
    "args.pp_folder = 'pp'\n",
    "args.monitor_train_every = 10\n",
    "args.save_every_epoch = False #!\n",
    "args.model = 'seq2seq_per_subgoal'\n",
    "args.gpu = True\n",
    "args.dout = 'exp/model:seq2seq_per_subgoal'\n",
    "args.resume = False #!\n",
    "\n",
    "# hyper parameters\n",
    "args.batch = 8\n",
    "args.epoch = 20\n",
    "args.lr = 1e-4\n",
    "args.decay_epoch = 10\n",
    "args.dhid = 512\n",
    "args.dframe = 2500\n",
    "args.demb = 100\n",
    "args.pframe = 300\n",
    "args.mask_loss_wt = 1.\n",
    "args.action_loss_wt = 1.\n",
    "args.subgoal_aux_loss_wt = 0.\n",
    "args.pm_aux_loss_wt = 0.\n",
    "\n",
    "# architecture ablations\n",
    "# args.maxpool_over_object_states = False\n",
    "# args.aux_loss_over_object_states = False\n",
    "args.encoder_addons = 'none'\n",
    "args.decoder_addons = 'none'\n",
    "args.object_repr = 'type'\n",
    "args.reweight_aux_bce = False\n",
    "\n",
    "# dropouts\n",
    "args.zero_goal = False #!\n",
    "args.zero_instr = False #!\n",
    "args.act_dropout = 0.\n",
    "args.lang_dropout = 0.\n",
    "args.input_dropout = 0.\n",
    "args.vis_dropout = 0.3\n",
    "args.hstate_dropout = 0.3\n",
    "args.attn_dropout = 0.\n",
    "args.actor_dropout = 0.\n",
    "args.word_dropout = 0.\n",
    "\n",
    "# other settings\n",
    "args.train_teacher_forcing = False #!\n",
    "args.train_student_forcing_prob = 0.1\n",
    "args.temp_no_history = False #!\n",
    "\n",
    "# debugging\n",
    "args.fast_epoch = False #!\n",
    "args.dataset_fraction = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the default flags\n",
    "\n",
    "args.preprocess = False # Turn this to True if running for the first time\n",
    "\n",
    "args.model = 'seq2seq_per_subgoal'  # found under models/model/ directory\n",
    "args.dout = '/root/data_alfred/exp/model:seq2seq_per_subgoal_biattn_instance_fast_epoch'\n",
    "\n",
    "args.train_teacher_forcing = True\n",
    "args.gpu = False\n",
    "\n",
    "# light setup for debugging\n",
    "args.fast_epoch = True # Turn this to False if running for the first time to preprocess data properly\n",
    "args.epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.maxpool_over_object_states = True\n",
    "# args.aux_loss_over_object_states = True\n",
    "\n",
    "args.encoder_addons = 'biattn_obj'\n",
    "args.decoder_addons = 'aux_loss'\n",
    "args.object_repr = 'instance'\n",
    "args.reweight_aux_bce = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd4300ad630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and load data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.out  /root/data_alfred/exp/model:seq2seq_per_subgoal_biattn_instance_fast_epoch\n"
     ]
    }
   ],
   "source": [
    "# make output dir\n",
    "if not os.path.isdir(args.dout):\n",
    "    os.makedirs(args.dout)\n",
    "\n",
    "print('args.out ', args.dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tests_seen': 1533,\n",
      " 'tests_unseen': 1529,\n",
      " 'train': 20806,\n",
      " 'train_sanity': 246,\n",
      " 'train_sanity_v1': 246,\n",
      " 'valid_seen': 814,\n",
      " 'valid_seen_v1': 249,\n",
      " 'valid_unseen': 818,\n",
      " 'valid_unseen_v1': 254}\n"
     ]
    }
   ],
   "source": [
    "# load train/valid/tests splits\n",
    "with open(args.splits) as f:\n",
    "    splits = json.load(f)\n",
    "    pprint.pprint({k: len(v) for k, v in splits.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_high': Vocab(93), 'word': Vocab(2360), 'action_low': Vocab(15)}\n"
     ]
    }
   ],
   "source": [
    "# preprocess and save -- only need to preprocess once\n",
    "if args.preprocess:\n",
    "    print(\"\\nPreprocessing dataset and saving to %s folders ... This will take a while. Do this once as required.\" % args.pp_folder)\n",
    "    dataset = Dataset(args, None)\n",
    "    dataset.preprocess_splits(splits)\n",
    "    vocab = torch.load(os.path.join(args.dout, \"%s.vocab\" % args.pp_folder))\n",
    "else:\n",
    "    vocab = torch.load(os.path.join(args.data, \"%s.vocab\" % args.pp_folder))\n",
    "    \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_vocab = torch.load(os.path.join(args.data, '%s.vocab' % args.object_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model architecture\n",
    "args.gpu = False\n",
    "args.resume = None # os.path.join(args.dout, 'best_seen.pth')\n",
    "\n",
    "M = import_module('model.{}'.format(args.model))\n",
    "if args.resume:\n",
    "    print(\"Loading: \" + args.resume)\n",
    "    model, optimizer, start_epoch = M.Module.load(args.resume)\n",
    "    print(\"Restarting at epoch {}/{}\".format(start_epoch, args.epoch-1))\n",
    "    if start_epoch >= args.epoch:\n",
    "        print('Checkpoint already finished {}/{} epochs.'.format(start_epoch, args.epoch))\n",
    "        sys.exit(0)\n",
    "else:\n",
    "    model = M.Module(args, vocab, object_vocab)\n",
    "    optimizer = None\n",
    "    start_epoch = 0\n",
    "\n",
    "if args.gpu:\n",
    "    model = model.to(torch.device('cuda'))\n",
    "    if not optimizer is None:\n",
    "        optimizer_to(optimizer, torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (emb_word): Embedding(2360, 100)\n",
       "  (emb_action_low): Embedding(15, 100)\n",
       "  (emb_object): Embedding(111, 100, padding_idx=0)\n",
       "  (instance_fc): Linear(in_features=201, out_features=512, bias=True)\n",
       "  (enc): ActionFrameAttnEncoderPerSubgoalObjAttn(\n",
       "    (emb): Embedding(15, 100)\n",
       "    (vis_dropout): Dropout(p=0.3, inplace=False)\n",
       "    (act_dropout): Dropout(p=0.0, inplace=True)\n",
       "    (vis_encoder): ResnetVisualEncoder(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fc): Linear(in_features=3136, out_features=2500, bias=True)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (enc_att): SelfAttn(\n",
       "      (scorer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "    (encoder): LSTM(2600, 512, batch_first=True, bidirectional=True)\n",
       "    (input_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hstate_dropout): Dropout(p=0.3, inplace=False)\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (obj_emb): Embedding(111, 100, padding_idx=0)\n",
       "    (instance_fc): Linear(in_features=201, out_features=512, bias=True)\n",
       "    (obj_attn): DotAttn()\n",
       "    (h_tm1_fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (forward_cell): LSTMCell(3624, 512)\n",
       "    (backward_cell): LSTMCell(3624, 512)\n",
       "  )\n",
       "  (dec): LanguageDecoder(\n",
       "    (emb): Embedding(2360, 100)\n",
       "    (obj_emb): Embedding(111, 100, padding_idx=0)\n",
       "    (cell): LSTMCell(1124, 1024)\n",
       "    (attn): DotAttn()\n",
       "    (input_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hstate_dropout): Dropout(p=0.3, inplace=False)\n",
       "    (word_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (word): Linear(in_features=2148, out_features=100, bias=True)\n",
       "    (h_tm1_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (h_enc_fc): Linear(in_features=1, out_features=2, bias=True)\n",
       "    (instance_fc): Linear(in_features=201, out_features=512, bias=True)\n",
       "  )\n",
       "  (vis_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (act_dropout): Dropout(p=0.0, inplace=True)\n",
       "  (aux_criterion): BCEWithLogitsLoss()\n",
       "  (language_crtierion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine model layers\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/50 [00:00<?, ?it/s]\n",
      "batch:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /root/data_alfred/exp/model:seq2seq_per_subgoal_biattn_instance_fast_epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py:297: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  empty_tensor = torch.ones(torch.tensor(v[0][0][0]).unsqueeze(0).shape, device=device, dtype=torch.float if ('frames' in k) else torch.long) * self.pad\n",
      "/root/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seqs.append(torch.tensor(v[subgoal_i][batch_i], device=device, dtype=torch.float if ('frames' in k) else torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /root/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py(607)compute_aux_loss()\n",
      "-> return torch.sum(loss_per_task), num_valid_tasks\n",
      "(Pdb) loss_per_task_pos\n",
      "tensor([7.2773e-05, 7.4698e-01, 2.2028e+00, 7.2935e-01, 7.7311e-01, 2.1730e+00,\n",
      "        7.9567e-05, 7.0508e-05], grad_fn=<DivBackward0>)\n",
      "(Pdb) loss_per_task_neg\n",
      "tensor([4.8990e+00, 6.2059e+00, 4.9066e+00, 6.2747e+00, 2.9781e+04, 4.8736e+00,\n",
      "        4.8807e+00, 4.9164e+00], grad_fn=<DivBackward0>)\n",
      "(Pdb) loss_pos\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0490e-05,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3506e-04, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7751e-06, 1.1086e-05, 1.2517e-05, 1.2517e-05,\n",
      "         1.1921e-05, 2.1473e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0072e-04,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0757e+00, 0.0000e+00, 1.2398e-05,\n",
      "         0.0000e+00, 1.9780e+00, 0.0000e+00, 0.0000e+00, 4.4070e-02, 0.0000e+00,\n",
      "         1.0097e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2960e+00, 4.1938e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.0599e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.8795e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-05, 9.1483e-03, 2.8017e-03,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.8683e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7376e-02, 2.7261e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.4996e+01, 0.0000e+00, 7.4549e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.1568e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7751e-06, 1.1086e-05, 1.2636e-05, 1.2636e-05,\n",
      "         1.2040e-05, 2.5129e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2634e-04,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8335e+00, 0.0000e+00, 1.1086e-05,\n",
      "         0.0000e+00, 1.9357e+00, 0.0000e+00, 0.0000e+00, 4.9752e-02, 0.0000e+00,\n",
      "         7.7841e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3290e+00, 3.7365e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1342e-02, 1.8090e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7037e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 6.3488e-02, 0.0000e+00, 0.0000e+00, 1.1563e-05, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7511e-03,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7087e-05, 2.1908e-04,\n",
      "         3.4133e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6432e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.3596e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6045e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1682e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.7699e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0014e-05, 1.0693e-02, 2.2205e-03,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.2392e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6547e-02, 2.6436e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.4810e+01, 0.0000e+00, 8.0804e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0255e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2159e-05,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4697e-04, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0133e-05,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3088e-04, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) torch.sum(targets)\n",
      "tensor(68.)\n",
      "(Pdb) (1.0 - targets)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "(Pdb) loss_full\n",
      "tensor([[4.4963e+00, 4.5422e+00, 1.0977e+01, 1.1059e+01, 1.1162e+01, 1.0490e-05,\n",
      "         1.1767e+01, 5.1465e+00, 1.9488e-01, 1.0982e+01, 6.4559e-01, 8.6575e-01,\n",
      "         1.4502e-02, 1.4604e-02, 1.3921e-01, 7.3765e-01, 7.5938e+00, 7.7718e+00,\n",
      "         2.7239e+00, 2.9025e+00, 1.0814e+01, 1.3506e-04, 1.6756e+01, 1.4578e+01,\n",
      "         2.9570e-03, 2.8300e+00, 2.8328e+00, 3.0316e-02, 2.2650e-05, 4.6727e-02,\n",
      "         6.4373e-06, 7.9800e-01, 9.1809e-04, 1.2704e-03, 1.5660e-03, 1.0876e+01,\n",
      "         1.4027e-03, 1.7644e+00, 5.3735e+00, 1.2387e+01, 1.8232e+01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4350e+00, 4.4447e+00, 9.7751e-06, 1.1086e-05, 1.2517e-05, 1.2517e-05,\n",
      "         1.1921e-05, 2.1473e-02, 1.1310e+01, 1.1419e+01, 1.1457e+01, 3.0072e-04,\n",
      "         1.1261e+01, 1.0221e-03, 2.0610e-02, 4.0757e+00, 4.7684e-07, 1.2398e-05,\n",
      "         5.7109e+00, 1.9780e+00, 8.4538e-01, 7.8792e+00, 4.4070e-02, 1.0530e+01,\n",
      "         1.0097e-04, 2.3803e-01, 1.6896e+01, 1.4865e+01, 4.2960e+00, 4.1938e-02,\n",
      "         7.0166e-01, 1.9671e-02, 7.0526e-02, 6.4373e-06, 6.1989e-06, 6.2482e+00,\n",
      "         1.1540e+01, 2.8713e-01, 3.1371e-01, 2.0662e+00, 5.6685e+00, 1.1890e+01,\n",
      "         1.7748e+01, 1.8307e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.5426e+00, 9.9654e-05, 6.9990e-01, 1.8523e-01, 1.6139e+00, 1.1875e+01,\n",
      "         1.2052e+01, 1.1948e+01, 1.1791e+01, 1.2184e+01, 1.1969e+01, 1.2045e+01,\n",
      "         1.1861e+01, 9.0599e-06, 1.1605e+01, 1.1821e+01, 1.1817e+01, 1.1503e+01,\n",
      "         5.0945e+00, 8.0931e+00, 3.8795e+00, 2.3130e+00, 3.9000e-01, 3.5082e-01,\n",
      "         1.1287e+01, 3.4187e-02, 2.4506e-04, 1.1921e-05, 9.1483e-03, 2.8017e-03,\n",
      "         4.6012e+00, 2.8135e+00, 7.3836e+00, 1.1280e+00, 7.9798e+00, 8.2025e+00,\n",
      "         7.9456e+00, 1.9431e-05, 1.5497e-05, 1.0268e-03, 1.1113e+01, 1.1658e+01,\n",
      "         6.7856e-03, 1.0945e-03, 3.4074e-01, 1.6134e-02, 1.6382e-02, 2.4856e-03,\n",
      "         2.5318e-02, 2.5130e-02, 1.8117e-03, 4.8411e-04, 5.1306e-04, 5.5984e+00,\n",
      "         3.8683e-01, 1.9863e+00, 1.9636e+00, 1.9568e+00, 2.7376e-02, 2.7261e-02,\n",
      "         6.7220e-02, 0.0000e+00, 1.4996e+01, 3.6095e+00, 7.4549e-01, 2.7801e+00,\n",
      "         2.8414e+00, 2.8768e+00, 2.9328e+00, 1.1895e+01, 1.1912e+01, 1.1945e+01,\n",
      "         1.1972e+01, 1.3362e-03, 4.1568e+00, 8.2162e-02, 3.9105e-03, 1.8468e+01,\n",
      "         3.2048e+00, 3.2761e+00, 4.7513e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3662e+00, 4.3763e+00, 9.7751e-06, 1.1086e-05, 1.2636e-05, 1.2636e-05,\n",
      "         1.2040e-05, 2.5129e-02, 1.1304e+01, 1.1417e+01, 1.1456e+01, 3.2634e-04,\n",
      "         1.1302e+01, 1.1176e-03, 2.6472e-02, 3.8335e+00, 4.7684e-07, 1.1086e-05,\n",
      "         5.9142e+00, 1.9357e+00, 9.9915e-01, 8.0758e+00, 4.9752e-02, 1.0756e+01,\n",
      "         7.7841e-05, 2.1235e-01, 1.6979e+01, 1.5012e+01, 4.3290e+00, 3.7365e-02,\n",
      "         8.3830e-01, 2.5417e-02, 6.4345e-02, 5.9604e-06, 5.7220e-06, 6.3548e+00,\n",
      "         1.1524e+01, 2.8022e-01, 3.0717e-01, 2.1453e+00, 5.8104e+00, 1.2172e+01,\n",
      "         1.7920e+01, 1.8602e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.9739e+00, 2.1342e-02, 1.8090e-02, 8.5946e-05, 2.0912e+00, 6.0456e-01,\n",
      "         3.5536e-02, 3.7360e-02, 3.7775e-01, 1.7638e+00, 1.7037e-01, 1.1524e+01,\n",
      "         1.1793e+01, 1.1282e+01, 1.1267e+01, 1.1127e+01, 1.1518e+01, 1.1501e+01,\n",
      "         1.1799e+01, 1.1556e+01, 1.1442e+01, 1.1564e+01, 1.1787e+01, 1.1456e+01,\n",
      "         1.1667e+01, 1.1874e+01, 1.1620e+01, 1.1784e+01, 1.1798e+01, 1.1668e+01,\n",
      "         1.1561e+01, 1.1833e+00, 5.4097e+00, 8.1068e+00, 1.7390e-02, 2.3388e-02,\n",
      "         1.8488e-04, 6.3488e-02, 2.1014e-04, 1.1212e+01, 1.1563e-05, 4.9051e-03,\n",
      "         6.9517e-03, 1.1296e+01, 1.1188e+01, 1.1109e+01, 1.1425e+01, 1.1478e+01,\n",
      "         1.1591e+01, 3.5340e+00, 1.7671e-01, 4.8204e+00, 1.1099e+00, 6.7511e-03,\n",
      "         7.8549e+00, 6.8347e-01, 1.1471e-01, 3.5597e-03, 4.7087e-05, 2.1908e-04,\n",
      "         3.4133e+00, 4.0410e+05, 4.0410e+05, 4.0410e+05, 4.0410e+05, 4.0410e+05,\n",
      "         4.0410e+05, 4.0410e+05, 1.1328e+01, 1.1665e+01, 4.6432e+00, 3.6078e-01,\n",
      "         1.9611e-02, 2.0065e-02, 2.2806e-03, 3.3766e-04, 6.0743e-04, 5.9321e+00,\n",
      "         5.9534e+00, 1.1266e+00, 3.6010e+00, 3.6032e+00, 6.3181e-06, 8.4270e-02,\n",
      "         9.3596e-01, 1.3351e-05, 1.6689e-06, 3.5155e+00, 1.5449e+00, 3.5596e+00,\n",
      "         2.7144e+00, 2.7428e+00, 2.8447e+00, 2.7665e+00, 2.8648e+00, 1.2000e+01,\n",
      "         1.2005e+01, 1.2021e+01, 1.2031e+01, 7.7456e-04, 1.3114e-01, 1.6482e-02,\n",
      "         1.7949e+01, 1.8259e+01, 2.1543e-03, 2.4144e-03, 4.6045e-03],\n",
      "        [3.4264e+00, 8.7138e-05, 5.6070e-01, 1.5060e-01, 1.4251e+00, 1.1621e+01,\n",
      "         1.1798e+01, 1.1694e+01, 1.1537e+01, 1.1930e+01, 1.1714e+01, 1.1791e+01,\n",
      "         1.1607e+01, 1.1682e-05, 1.1351e+01, 1.1567e+01, 1.1563e+01, 1.1250e+01,\n",
      "         5.1602e+00, 8.2250e+00, 3.7699e+00, 2.1436e+00, 3.9142e-01, 3.5217e-01,\n",
      "         1.1263e+01, 3.5644e-02, 2.8916e-04, 1.0014e-05, 1.0693e-02, 2.2205e-03,\n",
      "         4.5316e+00, 2.8172e+00, 7.5423e+00, 9.4263e-01, 7.8927e+00, 8.1150e+00,\n",
      "         7.8586e+00, 1.7643e-05, 1.4067e-05, 1.4145e-03, 1.1360e+01, 1.1672e+01,\n",
      "         6.6074e-03, 1.1476e-03, 3.4404e-01, 1.7619e-02, 1.7889e-02, 2.9250e-03,\n",
      "         2.1531e-02, 2.1371e-02, 2.6219e-03, 7.9052e-04, 8.3781e-04, 5.6536e+00,\n",
      "         4.2392e-01, 2.2322e+00, 2.2087e+00, 2.2017e+00, 2.6547e-02, 2.6436e-02,\n",
      "         7.6221e-02, 0.0000e+00, 1.4810e+01, 3.4367e+00, 8.0804e-01, 2.9188e+00,\n",
      "         2.9805e+00, 3.0161e+00, 3.0725e+00, 1.1802e+01, 1.1819e+01, 1.1853e+01,\n",
      "         1.1879e+01, 1.2951e-03, 4.0255e+00, 8.7000e-02, 5.0395e-03, 1.8449e+01,\n",
      "         3.3930e+00, 3.4647e+00, 4.8719e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4931e+00, 4.5393e+00, 1.0826e+01, 1.0908e+01, 1.1013e+01, 1.2159e-05,\n",
      "         1.1620e+01, 5.3613e+00, 2.2975e-01, 1.1047e+01, 6.7904e-01, 7.8188e-01,\n",
      "         1.5694e-02, 1.7334e-02, 1.4169e-01, 7.1686e-01, 7.6758e+00, 7.8548e+00,\n",
      "         2.7469e+00, 2.9267e+00, 1.0758e+01, 1.4697e-04, 1.6788e+01, 1.4497e+01,\n",
      "         3.4025e-03, 2.6954e+00, 2.6982e+00, 3.3864e-02, 3.0398e-05, 4.0192e-02,\n",
      "         6.1989e-06, 7.1706e-01, 1.1008e-03, 1.5257e-03, 1.8827e-03, 1.0742e+01,\n",
      "         1.6290e-03, 1.9752e+00, 5.3727e+00, 1.2322e+01, 1.8107e+01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.5788e+00, 4.6253e+00, 1.0999e+01, 1.1082e+01, 1.1187e+01, 1.0133e-05,\n",
      "         1.1800e+01, 5.0849e+00, 2.2725e-01, 1.0870e+01, 5.8171e-01, 7.7797e-01,\n",
      "         1.6002e-02, 1.3163e-02, 1.1891e-01, 7.0451e-01, 7.7151e+00, 7.8955e+00,\n",
      "         2.8251e+00, 3.0072e+00, 1.0888e+01, 1.3088e-04, 1.6779e+01, 1.4637e+01,\n",
      "         2.5189e-03, 2.9054e+00, 2.9082e+00, 2.6930e-02, 2.5630e-05, 3.5952e-02,\n",
      "         6.6757e-06, 6.6473e-01, 1.1496e-03, 1.5976e-03, 1.9747e-03, 1.0756e+01,\n",
      "         1.4290e-03, 1.7659e+00, 5.4869e+00, 1.2598e+01, 1.8168e+01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) torch.sum(loss_per_task_pos)\n",
      "tensor(6.6255, grad_fn=<SumBackward0>)\n",
      "(Pdb) torch.sum(loss_per_task_neg)\n",
      "tensor(29817.9102, grad_fn=<SumBackward0>)\n",
      "(Pdb) loss_per_task\n",
      "tensor([2.4496e+00, 3.4764e+00, 3.5547e+00, 3.5020e+00, 1.4891e+04, 3.5233e+00,\n",
      "        2.4404e+00, 2.4582e+00], grad_fn=<DivBackward0>)\n",
      "(Pdb) q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:   0%|          | 0/2 [1:35:40<?, ?it/s]\n",
      "epoch:   0%|          | 0/50 [1:35:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2a79b170a1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main training loop -- debug here if breakpoints were inserted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/data/home/hoyeung/alfred/models/model/seq2seq.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self, splits, args, optimizer, start_epoch, end_epoch, start_iters)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'batch_loss_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, out, batch, feat)\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_visibility'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubgoal_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_low_seq_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                     )\n\u001b[0;32m--> 653\u001b[0;31m                 state_change_loss_per_subgoal[subgoal_i], tasks_per_subgoal_state_change = self.compute_aux_loss(\n\u001b[0m\u001b[1;32m    654\u001b[0m                     \u001b[0mvalid_object_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_token_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubgoal_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0mpredicted_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_state_change_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubgoal_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py\u001b[0m in \u001b[0;36mcompute_aux_loss\u001b[0;34m(self, valid_object_indices, predicted_scores, targets)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_per_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_valid_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         '''\n",
      "\u001b[0;32m~/data/home/hoyeung/alfred/models/model/seq2seq_per_subgoal.py\u001b[0m in \u001b[0;36mcompute_aux_loss\u001b[0;34m(self, valid_object_indices, predicted_scores, targets)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_per_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_valid_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         '''\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main training loop -- debug here if breakpoints were inserted\n",
    "model.run_train(splits, optimizer=optimizer, start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# old branch per-subgoal BASELINE\n",
    "\n",
    "{'epoch': 0,\n",
    " 'train_sanity': {'BLEU': 2.638813586708307e-232,\n",
    "                  'perplexity': 175.3822250366211,\n",
    "                  'total_loss': 7.454048156738281},\n",
    " 'valid_seen': {'BLEU': 2.7649164470978882e-232,\n",
    "                'perplexity': 180.12401580810547,\n",
    "                'total_loss': 7.491679668426514},\n",
    " 'valid_unseen': {'BLEU': 2.3879481169065848e-232,\n",
    "                  'perplexity': 160.17532348632812,\n",
    "                  'total_loss': 7.323445796966553}}\n",
    "epoch_time                    118.513                                 \n",
    "compute_metrics_validation_sets105.05                                  \n",
    "forward_batch_train_with_iterate12.287                                  \n",
    "forward_batch_train           11.946                                  \n",
    "iterate_featurize             0.326                                   \n",
    "torch_save_valid_seen         0.261                                   \n",
    "torch_save_last               0.252                                   \n",
    "torch_save_valid_unseen       0.247                                   \n",
    "torch_save_train_sanity       0.24                                    \n",
    "iterate_load_task_json        0.175                                   \n",
    "featurize_tensorization_and_padding0.158                                   \n",
    "featurize_input_resnet_features0.143                                   \n",
    "make_debug_valid_unseen       0.091                                   \n",
    "featurize_torch_load_time     0.09                                    \n",
    "make_debug_valid_seen         0.041                                   \n",
    "make_debug_train              0.041                                   \n",
    "setup_time                    0.001                                   \n",
    "featurize_input_action_low    0.0                                     \n",
    "compute_metrics_train         0.0                                     \n",
    "featurize_outputs             0.0 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'epoch': 0,\n",
    " 'train_sanity': {'BLEU': 1.8747843191033304e-232,\n",
    "                  'perplexity': 176.1246566772461,\n",
    "                  'total_loss': 7.460198402404785},\n",
    " 'valid_seen': {'BLEU': 2.5034942568793152e-232,\n",
    "                'perplexity': 180.14256286621094,\n",
    "                'total_loss': 7.49180793762207},\n",
    " 'valid_unseen': {'BLEU': 2.1108175036585326e-232,\n",
    "                  'perplexity': 160.90531158447266,\n",
    "                  'total_loss': 7.330006122589111}}\n",
    "epoch_time                    229.403                                 \n",
    "compute_metrics_validation_sets193.46                                  \n",
    "forward_batch_train_with_iterate34.642                                  \n",
    "forward_batch_train           34.261                                  \n",
    "iterate_featurize             0.324                                   \n",
    "torch_save_valid_seen         0.274                                   \n",
    "torch_save_train_sanity       0.267                                   \n",
    "torch_save_last               0.263                                   \n",
    "torch_save_valid_unseen       0.262                                   \n",
    "iterate_load_task_json        0.216                                   \n",
    "featurize_input_resnet_features0.165                                   \n",
    "featurize_tensorization_and_padding0.158                                   \n",
    "make_debug_valid_seen         0.097                                   \n",
    "make_debug_train              0.092                                   \n",
    "featurize_torch_load_time     0.088                                   \n",
    "make_debug_valid_unseen       0.043                                   \n",
    "setup_time                    0.001                                   \n",
    "featurize_input_action_low    0.001                                   \n",
    "compute_metrics_train         0.0                                     \n",
    "featurize_outputs             0.0 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# old branch BI-ATTN + AUX LOSS\n",
    "\n",
    "{'epoch': 0,\n",
    " 'train_sanity': {'BLEU': 5.272791609271694e-157,\n",
    "                  'perplexity': 133.40389251708984,\n",
    "                  'total_loss': 15.418501377105713},\n",
    " 'valid_seen': {'BLEU': 1.2697489715465426e-157,\n",
    "                'perplexity': 135.36600494384766,\n",
    "                'total_loss': 15.778061866760254},\n",
    " 'valid_unseen': {'BLEU': 1.3425960742964622e-157,\n",
    "                  'perplexity': 127.78440856933594,\n",
    "                  'total_loss': 15.614015579223633}}\n",
    "epoch_time                    237.858                                 \n",
    "compute_metrics_validation_sets204.668                                 \n",
    "forward_batch_train_with_iterate31.603                                  \n",
    "forward_batch_train           31.278                                  \n",
    "torch_save_train_sanity       0.344                                   \n",
    "iterate_featurize             0.34                                    \n",
    "torch_save_last               0.339                                   \n",
    "torch_save_valid_unseen       0.336                                   \n",
    "torch_save_valid_seen         0.327                                   \n",
    "featurize_tensorization_and_padding0.183                                   \n",
    "iterate_load_task_json        0.167                                   \n",
    "featurize_input_resnet_features0.134                                   \n",
    "make_debug_valid_unseen       0.101                                   \n",
    "make_debug_valid_seen         0.097                                   \n",
    "featurize_torch_load_time     0.076                                   \n",
    "make_debug_train              0.041                                   \n",
    "setup_time                    0.001                                   \n",
    "featurize_input_action_low    0.0                                     \n",
    "featurize_outputs             0.0                                     \n",
    "compute_metrics_train         0.0  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BI-ATTN + AUX LOSS\n",
    "\n",
    "{'epoch': 0,\n",
    " 'train_sanity': {'BLEU': 5.272791609271694e-157,\n",
    "                  'perplexity': 133.4038314819336,\n",
    "                  'total_loss': 15.418500900268555},\n",
    " 'valid_seen': {'BLEU': 1.2697489715465426e-157,\n",
    "                'perplexity': 135.36604690551758,\n",
    "                'total_loss': 15.778063297271729},\n",
    " 'valid_unseen': {'BLEU': 1.3425960742964622e-157,\n",
    "                  'perplexity': 127.78447341918945,\n",
    "                  'total_loss': 15.614017486572266}}\n",
    "epoch_time                    255.655                                 \n",
    "compute_metrics_validation_sets219.629                                 \n",
    "forward_batch_train_with_iterate34.462                                  \n",
    "forward_batch_train           34.142                                  \n",
    "torch_save_train_sanity       0.33                                    \n",
    "torch_save_valid_unseen       0.326                                   \n",
    "torch_save_last               0.326                                   \n",
    "torch_save_valid_seen         0.324                                   \n",
    "iterate_featurize             0.318                                   \n",
    "iterate_load_task_json        0.172                                   \n",
    "featurize_tensorization_and_padding0.162                                   \n",
    "featurize_input_resnet_features0.131                                   \n",
    "make_debug_valid_seen         0.105                                   \n",
    "make_debug_valid_unseen       0.093                                   \n",
    "featurize_torch_load_time     0.075                                   \n",
    "make_debug_train              0.053                                   \n",
    "setup_time                    0.001                                   \n",
    "compute_metrics_train         0.001                                   \n",
    "featurize_input_action_low    0.0                                     \n",
    "featurize_outputs             0.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BI-ATTN + AUX LOSS\n",
    "# INSTANCE BASED\n",
    "\n",
    "{'epoch': 0,\n",
    " 'train_sanity': {'BLEU': 2.1643244568036305e-232,\n",
    "                  'perplexity': 256.3981170654297,\n",
    "                  'total_loss': 314.7671890258789},\n",
    " 'valid_seen': {'BLEU': 2.3009385374372878e-232,\n",
    "                'perplexity': 272.8842468261719,\n",
    "                'total_loss': 669.7962036132812},\n",
    " 'valid_unseen': {'BLEU': 2.33772413597026e-232,\n",
    "                  'perplexity': 273.24156188964844,\n",
    "                  'total_loss': 183.23194122314453}}\n",
    "epoch_time                    213.74                                  \n",
    "compute_metrics_validation_sets174.366                                 \n",
    "forward_batch_train_with_iterate37.815                                  \n",
    "forward_batch_train           37.342                                  \n",
    "iterate_featurize             0.563                                   \n",
    "featurize_tensorization_and_padding0.371                                   \n",
    "torch_save_train_sanity       0.357                                   \n",
    "torch_save_valid_unseen       0.343                                   \n",
    "torch_save_valid_seen         0.337                                   \n",
    "torch_save_last               0.336                                   \n",
    "iterate_load_task_json        0.19                                    \n",
    "featurize_input_resnet_features0.129                                   \n",
    "make_debug_valid_unseen       0.099                                   \n",
    "featurize_torch_load_time     0.074                                   \n",
    "make_debug_valid_seen         0.042                                   \n",
    "make_debug_train              0.042                                   \n",
    "setup_time                    0.001                                   \n",
    "featurize_input_action_low    0.0                                     \n",
    "compute_metrics_train         0.0                                     \n",
    "featurize_outputs             0.0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
